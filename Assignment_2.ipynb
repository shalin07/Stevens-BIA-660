{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze a numpy array\n",
    " - Assume we have an array (with shape (M,N)) which contains term frequency of each document, where each row is a document, each column is a word, and the corresponding value denotes the frequency of the word in the document. Define a function named \"analyze_tf_idf\" which:\n",
    "      * takes the **array**, and an integer **K** as the parameters.\n",
    "      * normalizes the frequency of each word as: word frequency divided by the length of the document. Save the result as an array named **tf** (i.e. term frequency)\n",
    "      * calculates the document frequency (**df**) of each word, e.g. how many documents contain a specific word\n",
    "      * calculates **tf_idf** array as: **tf / (log(df)+1)** (tf divided by log(df)). The reason is, if a word appears in most documents, it does not have the discriminative power and often is called a \"stop\" word. The inverse of df can downgrade the weight of such words.\n",
    "      * for each document, finds out the **indexes of words with top K largest values in the tf_idf array**, ($0<K<=N$). These indexes form an array, say **top_K**, with shape (M, K)\n",
    "      * returns the tf_idf array, and the top_K array.\n",
    " - Note, for all the steps, ** do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a function to analyze stackoverflow dataset using pandas\n",
    " - Define a function named \"analyze_data\" to do the follows:\n",
    "   * Take a csv file path string as an input. Assume the csv file is in the format of the provided sample file (question.csv).\n",
    "   * Read the csv file as a dataframe with the first row as column names\n",
    "   * Find questions with top 3 viewcounts among those answered questions (i.e answercount>0). Print the title and viewcount columns of these questions.\n",
    "   * Find the top 5 users (i.e. quest_name) who asked the most questions.\n",
    "   * Create a new column called \"first_tag\" to store the very first tag in the \"tags\" column (hint: use \"apply\" function; tags are separted by \", \")\n",
    "   * Show the mean, min, and max viewcount values for each of these tags: \"python\", \"pandas\" and \"dataframe\"\n",
    "   * Create a cross tab with answercount as row indexes, first_tag as column names, and the count of samples as the value. For \"python\" question (i.e. first_tag=\"python\"), how many questions were not answered (i.e., answercount=0), how many questions were answered once (i.e., answercount=1), and how many questions were anasered twice  (i.e., answercount=2)? Print these numbers.\n",
    " - This function does not have any return. Just print out the result of each calculation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (Bonus). Analyzed a collection of documents\n",
    " - Define a function named \"analyze_corpus\" to do the follows:\n",
    "   * Similar to Q2, take a csv file path string as an input. Assume the csv file is in the format of the provided sample file (question.csv).\n",
    "   * Read the \"title\" column from the csv file and convert it to lower case\n",
    "   * Split each string in the \"title\" column by space to get tokens. Create an array where each row represents a title, each column denotes a unique token, and each value denotes the count of the token in the document\n",
    "   * Call your function in Q1 (i.e. analyze_tf_idf) to analyze this array\n",
    "   * Print out the top 5 words by tf-idf score for the first 20 questions. Do you think these top words allow you to find similar questions or differentiate a question from dissimilar ones? Write your analysis as a pdf file.\n",
    "   \n",
    "- This function does not have any return. Just print out the result if asked.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guideline##\n",
    "- Following the solution template provided below. Use __main__ block to test your functions\n",
    "- Save your code into a python file (e.g. assign2.py) that can be run in a python 3 environment. In Jupyter Notebook, you can export notebook as .py file in menu \"File->Download as\".\n",
    "- Make sure you have all import statements. To test your code, open a command window in your current python working folder, type \"python assign2.py\" to see if it can run successfully.\n",
    "- **Each homework assignment should be completed independently. Never ever copy others' work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1\n",
      "\u001b[1m\u001b[34mIndixes of top K values from array\u001b[0m\n",
      "[[3 1 5]\n",
      " [4 0 2]\n",
      " [2 5 0]]\n",
      "\n",
      "Q2\n",
      "\u001b[1m\u001b[34mTop 3 viewcounts where answercount is greater than 0\u001b[0m\n",
      "                                                 title  viewcount\n",
      "75   Python: Pandas pd.read_excel giving ImportErro...      33297\n",
      "163                     Python convert object to float      16658\n",
      "886                  Subtract two columns in dataframe      11176\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[34mTop 5 users who has asked most freqent ques\u001b[0m\n",
      "quest_name\n",
      "Rahul rajan     7\n",
      "Shuvayan Das    7\n",
      "Danny W         6\n",
      "el323           6\n",
      "Hana            5\n",
      "Name: id, dtype: int64\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[34mMin, Max and Mean of viewcount of each first_tag 'python', 'pandas' & 'dataframe'\u001b[0m\n",
      "           amin   amax        mean\n",
      "first_tag                         \n",
      "pandas       14   4499  454.687500\n",
      "python        5  33297  428.670091\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[34mCrosstab with answercount as row indexes and first_tag as column names\u001b[0m\n",
      "first_tag    arrays  c++  django  excel  function  json  machine-learning  \\\n",
      "answercount                                                                 \n",
      "0                 0    0       0      1         0     0                 0   \n",
      "1                 1    1       0      1         1     0                 0   \n",
      "2                 0    0       1      1         0     0                 1   \n",
      "3                 0    0       0      0         0     1                 0   \n",
      "4                 0    0       0      0         0     0                 0   \n",
      "5                 0    0       0      0         0     0                 0   \n",
      "7                 0    0       0      0         0     0                 0   \n",
      "\n",
      "first_tag    pandas  postgresql  python  python-2.7  python-3.x  regex  sql  \\\n",
      "answercount                                                                   \n",
      "0                11           0      98           1           9      0    0   \n",
      "1                37           1     455           5          20      1    1   \n",
      "2                13           0     232           3           6      0    0   \n",
      "3                 2           0      64           0           3      0    0   \n",
      "4                 1           0      21           0           0      0    0   \n",
      "5                 0           0       3           0           0      0    0   \n",
      "7                 0           0       3           0           0      0    0   \n",
      "\n",
      "first_tag    xml  \n",
      "answercount       \n",
      "0              1  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "5              0  \n",
      "7              0  \n",
      "None\n",
      "\n",
      "Q3\n",
      "\u001b[1m\u001b[34mCalling analyze_tf_idf function to analyze array\u001b[0m\n",
      "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[  78,  320,  633,  962,    0],\n",
      "       [ 784,  716,  443,  962, 1011],\n",
      "       [ 982, 1463, 1511,  863,  301],\n",
      "       ...,\n",
      "       [ 621, 1346,  586,  286, 1153],\n",
      "       [ 232, 1194,  223,  320, 1378],\n",
      "       [ 223,  813,  877,  920, 1147]], dtype=int64))\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[34mtop 5 words for the first 20 question\u001b[0m\n",
      "['array', 'in', 'dataframe', 'pandas', '0x7fbfe6954470']\n",
      "['encoding', 'manual', 'label', 'pandas', 'plus']\n",
      "['name', 'pathway', 'write', 'variable', 'csv']\n",
      "['google', 'finance', 'index', 'adding', 'time']\n",
      "['values', 'representation', 'convert', 'numeric', 'how']\n",
      "['create', 'from', 'dictionary', 'of', 'python']\n",
      "['setting', 'under', 'way', 'clever', 'conditions']\n",
      "['misinterpreting', 'date', 'file', 'csv', 'column']\n",
      "['groups', 'across', 'defined', 'summing', 'up']\n",
      "['read', 'tdms', 'labview', 'file', 'with']\n",
      "['rewrite', 'can', 'function', 'code', 'times']\n",
      "['records', 'group', 'by', 'formatting', 'output']\n",
      "['text', 'processing', 'nltk', 'based', 'with']\n",
      "['each', 'calculating', 'percentile', 'device_id', 'particular']\n",
      "['transform', 'layout', 'another', 'to', 'dataframe']\n",
      "['dataframes', 'vectorized', 'different', 'operations', 'on']\n",
      "['names', 'select', 'similar', 'data', 'frame']\n",
      "['flask', 'template', 'as', 'json', 'to']\n",
      "['same', 'filter', 'the', 'operations', 'on']\n",
      "['binarizer', 'label', 'multiple', 'columns', 'precision']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# Structure of your solution to Assignment 1 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def analyze_data(filepath):\n",
    "    \n",
    "    # add your code here\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(colored(\"Top 3 viewcounts where answercount is greater than 0\" , \"blue\", attrs=['bold']))\n",
    "\n",
    "    #question with top 3 viewcount where answercount>0\n",
    "    print(data[data.answercount>0].nlargest(3,'viewcount')[['title', 'viewcount']])\n",
    "    print('\\n')\n",
    "\n",
    "    print(colored(\"Top 5 users who has asked most freqent ques\", 'blue', attrs=['bold']))\n",
    "\n",
    "    #top 3 users who has asked most questions\n",
    "    print(data.groupby('quest_name').count().nlargest(5, 'id')['id'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #adding row first_tag with first token of tags\n",
    "    data['first_tag'] = data['tags'].apply(lambda x:x.split(',')[0])\n",
    "\n",
    "    print(colored(\"Min, Max and Mean of viewcount of each first_tag 'python', 'pandas' & 'dataframe'\", 'blue', attrs=['bold']))\n",
    "\n",
    "    #getting min max and mean of viewcount of each first_tag\n",
    "    print(data.loc[data.first_tag.isin(['python', 'pandas', 'dataframe'])].groupby('first_tag').viewcount.agg([np.min, np.max, np.mean]))\n",
    "    print('\\n')\n",
    "\n",
    "    print(colored(\"Crosstab with answercount as row indexes and first_tag as column names\", 'blue', attrs=['bold']))\n",
    "    \n",
    "    #crosstab with answercount as row indexes and first_tag as column name\n",
    "    print(pd.crosstab(data.answercount,data.first_tag))\n",
    "\n",
    "    \n",
    "def analyze_tf_idf(arr,K):\n",
    "    \n",
    "    tf_ifd=None\n",
    "    top_k=None\n",
    "    \n",
    "#     summation of row\n",
    "    row_sum = np.sum(arr, axis=1)  \n",
    "#     normalized matrix by row\n",
    "    tf = arr / row_sum[:, np.newaxis]\n",
    "#     calculating how many documnets contains specific word\n",
    "    tf_1 = np.where(arr>0,1,0)\n",
    "    df = np.sum(tf_1, axis=0)\n",
    "#      checking for stop words\n",
    "    tf_idf = tf/(np.log(df)+1)\n",
    "#  \n",
    "    top_k = (-tf_idf).argsort()\n",
    "    return tf_idf, top_k[:,0:K]\n",
    "\n",
    "def analyze_corpus(filepath):\n",
    "    \n",
    "    # add your code here\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    #converting title column into lowercase\n",
    "    data1 = data['title'].str.lower().str.split()\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    #using countvectorizer library to get count of unique elements and there index\n",
    "    X = vectorizer.fit_transform(data['title'])\n",
    "    \n",
    "    #getiing list of all unique tokens\n",
    "    unique_tokens = vectorizer.get_feature_names()\n",
    "    \n",
    "    #converting into array\n",
    "    y = X.toarray()\n",
    "    \n",
    "    print(colored(\"Calling analyze_tf_idf function to analyze array\", 'blue', attrs=['bold']))\n",
    "    print(analyze_tf_idf(y,5))\n",
    "    print('\\n')\n",
    "    \n",
    "    tf_idf, top_k=analyze_tf_idf(y[0:20,],5)\n",
    "    print(colored(\"top 5 words for the first 20 question\", 'blue', attrs=['bold']))\n",
    "    for i in top_k:\n",
    "        list1=[]\n",
    "        for j in i:\n",
    "            list1.append(unique_tokens[j])\n",
    "        print(list1)\n",
    "    \n",
    "    \n",
    "\n",
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "if __name__ == \"__main__\":  \n",
    "\n",
    "        # Test Question 1\n",
    "    arr=np.array([[0,1,0,2,0,1],[1,0,1,1,2,0],[0,0,2,0,0,1]])\n",
    "\n",
    "    print(\"\\nQ1\")\n",
    "    print(colored(\"Indixes of top K values from array\", 'blue', attrs=['bold']))\n",
    "    tf_idf, top_k=analyze_tf_idf(arr,3)\n",
    "    print(top_k)\n",
    "\n",
    "    print(\"\\nQ2\")\n",
    "    print(analyze_data('../question.csv'))\n",
    "\n",
    "        # test question 3\n",
    "    print(\"\\nQ3\")\n",
    "    analyze_corpus('../question.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
