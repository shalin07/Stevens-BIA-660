{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze the frequency of words in a string ##\n",
    " - Define a function named \"**tokenize**\" which does the following:\n",
    "     * has a string as an input \n",
    "     * splits the string into a list of tokens by space. For example, \"it's hello world!\" will be split into two tokens [\"it's\", \"hello\",\"world!\"]\n",
    "     * removes all spaces around each token (including tabs, newline characters (\"\\n\"))\n",
    "     * if a token starts with or ends with a punctuation, remove the punctuation, e.g. \"world<font color=\"red\">!</font>\" -> \"world\",  \"<font color=\"red\">'</font>hello<font color=\"red\">'</font>\"->\"hello\" (<font color=\"blue\">hint, you can use *string.punctuation* to get a list of punctuations, where *string* is a module you can import</font>)\n",
    "     * removes empty tokens, i.e. *len*(token)==0\n",
    "     * converts all tokens into lower case\n",
    "     * returns all the tokens as a list output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a class to analyze a document ##\n",
    " - Define a new class called \"**Text_Analyzer**\" which does the following :\n",
    "    - has two attributes: \n",
    "        * **input_string**, which receives the string value passed by users when creating an object of this class.\n",
    "        * **token_count**, which is set to {} when an object of this class is created.\n",
    "        \n",
    "    - a function named \"**analyze**\" that does the following:\n",
    "      * calls the function \"tokenize\" to get a list of tokens. \n",
    "      * creates a dictionary containing the count of every unique token, e.g. {'it': 5, 'hello':1,...}\n",
    "      * saves this dictionary to the token_count attribute\n",
    "    - a function named \"**topN**\" that returns the top N words by frequency\n",
    "      * has a integer parameter *N*  \n",
    "      * returns the top *N* words and their counts as a list of tuples, e.g. [(\"hello\", 5), (\"world\", 4),...] (<font color=\"blue\">hint: By default, a dictionary is sorted by key. However, you need to sort the token_count dictionary by value</font>)\n",
    "  \n",
    "- What kind of words usually have high frequency? Write your analysis.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. (Bonus) Create Bigrams from a document ##\n",
    "\n",
    "A bigram is any pair of consecutive tokens in a document. Phrases are usually bigrams. Let's design a function to find phrases.\n",
    " - Create a new function called \"**bigram**\" which does the following :\n",
    "     * takes a **string** and an integer **N** as inputs\n",
    "     * calls the function \"tokenize\" to get a list of tokens for the input string\n",
    "     * slice the list to get any two consecutive tokens as a bigram. For example [\"it's\", \"hello\",\"world\"] will generate two bigrams: [[\"it's\", \"hello\"],[\"hello\",\"world\"]]\n",
    "     * count the frequency of each unique bigram\n",
    "     * return top N bigrams and their counts \n",
    " - Are you able to find good phrases from the top N bigrams? Write down your analysis in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guideline##\n",
    "- Following the solution template provided below. Use __main__ block to test your functions and class\n",
    "- Save your code into a python file (e.g. assign1.py) that can be run in a python 3 environment. In Jupyter Notebook, you can export notebook as .py file in menu \"File->Download as\".\n",
    "- Make sure you have all import statements. To test your code, open a command window in your current python working folder, type \"python assign1.py\" to see if it can run successfully.\n",
    "- For more details, check assignment submission guideline on Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mOutput of Question 1:\u001b[0m\n",
      "['there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', 'oh', 'dear', 'oh', 'dear', 'i', 'shall', 'be', 'late', 'when', 'she', 'thought', 'it', 'over', 'afterwards', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waistcoat', 'pocket', 'and', 'looked', 'at', 'it', 'and', 'then', 'hurried', 'on', 'alice', 'started', 'to', 'her', 'feet', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoatpocket', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', 'and', 'burning', 'with', 'curiosity', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbithole', 'under', 'the', 'hedge']\n",
      "\n",
      "\u001b[1m\u001b[32mOutput of question 2:\u001b[0m\n",
      "\u001b[1m\u001b[34mWords and count pair\u001b[0m\n",
      "Counter({'it': 9, 'to': 7, 'the': 6, 'a': 5, 'she': 4, 'and': 4, 'that': 3, 'out': 3, 'of': 3, 'rabbit': 3, 'her': 3, 'at': 3, 'was': 2, 'so': 2, 'very': 2, 'in': 2, 'alice': 2, 'oh': 2, 'dear': 2, 'when': 2, 'but': 2, 'time': 2, 'watch': 2, 'across': 2, 'with': 2, 'there': 1, 'nothing': 1, 'remarkable': 1, 'nor': 1, 'did': 1, 'think': 1, 'much': 1, 'way': 1, 'hear': 1, 'say': 1, 'itself': 1, 'i': 1, 'shall': 1, 'be': 1, 'late': 1, 'thought': 1, 'over': 1, 'afterwards': 1, 'occurred': 1, 'ought': 1, 'have': 1, 'wondered': 1, 'this': 1, 'all': 1, 'seemed': 1, 'quite': 1, 'natural': 1, 'actually': 1, 'took': 1, 'its': 1, 'waistcoat': 1, 'pocket': 1, 'looked': 1, 'then': 1, 'hurried': 1, 'on': 1, 'started': 1, 'feet': 1, 'for': 1, 'flashed': 1, 'mind': 1, 'had': 1, 'never': 1, 'before': 1, 'seen': 1, 'either': 1, 'waistcoatpocket': 1, 'or': 1, 'take': 1, 'burning': 1, 'curiosity': 1, 'ran': 1, 'field': 1, 'after': 1, 'fortunately': 1, 'just': 1, 'see': 1, 'pop': 1, 'down': 1, 'large': 1, 'rabbithole': 1, 'under': 1, 'hedge': 1})\n",
      "\n",
      "\u001b[1m\u001b[34mTop N Words\u001b[0m\n",
      "[('it', 9), ('to', 7), ('the', 6), ('a', 5), ('she', 4)]\n",
      "\n",
      "\u001b[1m\u001b[32mOutput of question 3:\u001b[0m\n",
      "\u001b[1m\u001b[34mBigram and count pair\u001b[0m\n",
      "Counter({('out', 'of'): 3, ('it', 'and'): 3, ('so', 'very'): 2, ('the', 'rabbit'): 2, ('oh', 'dear'): 2, ('to', 'her'): 2, ('that', 'she'): 2, ('a', 'watch'): 2, ('there', 'was'): 1, ('was', 'nothing'): 1, ('nothing', 'so'): 1, ('very', 'remarkable'): 1, ('remarkable', 'in'): 1, ('in', 'that'): 1, ('that', 'nor'): 1, ('nor', 'did'): 1, ('did', 'alice'): 1, ('alice', 'think'): 1, ('think', 'it'): 1, ('it', 'so'): 1, ('very', 'much'): 1, ('much', 'out'): 1, ('of', 'the'): 1, ('the', 'way'): 1, ('way', 'to'): 1, ('to', 'hear'): 1, ('hear', 'the'): 1, ('rabbit', 'say'): 1, ('say', 'to'): 1, ('to', 'itself'): 1, ('itself', 'oh'): 1, ('dear', 'oh'): 1, ('dear', 'i'): 1, ('i', 'shall'): 1, ('shall', 'be'): 1, ('be', 'late'): 1, ('late', 'when'): 1, ('when', 'she'): 1, ('she', 'thought'): 1, ('thought', 'it'): 1, ('it', 'over'): 1, ('over', 'afterwards'): 1, ('afterwards', 'it'): 1, ('it', 'occurred'): 1, ('occurred', 'to'): 1, ('her', 'that'): 1, ('she', 'ought'): 1, ('ought', 'to'): 1, ('to', 'have'): 1, ('have', 'wondered'): 1, ('wondered', 'at'): 1, ('at', 'this'): 1, ('this', 'but'): 1, ('but', 'at'): 1, ('at', 'the'): 1, ('the', 'time'): 1, ('time', 'it'): 1, ('it', 'all'): 1, ('all', 'seemed'): 1, ('seemed', 'quite'): 1, ('quite', 'natural'): 1, ('natural', 'but'): 1, ('but', 'when'): 1, ('when', 'the'): 1, ('rabbit', 'actually'): 1, ('actually', 'took'): 1, ('took', 'a'): 1, ('watch', 'out'): 1, ('of', 'its'): 1, ('its', 'waistcoat'): 1, ('waistcoat', 'pocket'): 1, ('pocket', 'and'): 1, ('and', 'looked'): 1, ('looked', 'at'): 1, ('at', 'it'): 1, ('and', 'then'): 1, ('then', 'hurried'): 1, ('hurried', 'on'): 1, ('on', 'alice'): 1, ('alice', 'started'): 1, ('started', 'to'): 1, ('her', 'feet'): 1, ('feet', 'for'): 1, ('for', 'it'): 1, ('it', 'flashed'): 1, ('flashed', 'across'): 1, ('across', 'her'): 1, ('her', 'mind'): 1, ('mind', 'that'): 1, ('she', 'had'): 1, ('had', 'never'): 1, ('never', 'before'): 1, ('before', 'seen'): 1, ('seen', 'a'): 1, ('a', 'rabbit'): 1, ('rabbit', 'with'): 1, ('with', 'either'): 1, ('either', 'a'): 1, ('a', 'waistcoatpocket'): 1, ('waistcoatpocket', 'or'): 1, ('or', 'a'): 1, ('watch', 'to'): 1, ('to', 'take'): 1, ('take', 'out'): 1, ('of', 'it'): 1, ('and', 'burning'): 1, ('burning', 'with'): 1, ('with', 'curiosity'): 1, ('curiosity', 'she'): 1, ('she', 'ran'): 1, ('ran', 'across'): 1, ('across', 'the'): 1, ('the', 'field'): 1, ('field', 'after'): 1, ('after', 'it'): 1, ('and', 'fortunately'): 1, ('fortunately', 'was'): 1, ('was', 'just'): 1, ('just', 'in'): 1, ('in', 'time'): 1, ('time', 'to'): 1, ('to', 'see'): 1, ('see', 'it'): 1, ('it', 'pop'): 1, ('pop', 'down'): 1, ('down', 'a'): 1, ('a', 'large'): 1, ('large', 'rabbithole'): 1, ('rabbithole', 'under'): 1, ('under', 'the'): 1, ('the', 'hedge'): 1})\n",
      "\n",
      "\u001b[1m\u001b[34mTop N bigram\u001b[0m\n",
      "[(('out', 'of'), 3), (('it', 'and'), 3), (('so', 'very'), 2), (('the', 'rabbit'), 2), (('oh', 'dear'), 2)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string \n",
    "from collections import Counter\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    tokens=[]\n",
    "    \n",
    "    tokens=[]\n",
    "    tokens = text.split()\n",
    "    tokens = [x.strip(' ') for x in tokens]\n",
    "    tokens = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "    tokens = list(filter(None, tokens))\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    return tokens\n",
    "    \n",
    "class Text_Analyzer(object):\n",
    "            \n",
    "        def __init__(self, text):\n",
    "            self.input_string = text\n",
    "            self.token_count = {}\n",
    "        \n",
    "          \n",
    "        def analyze(self):\n",
    "            c = tokenize(self.input_string)\n",
    "            self.token_count = Counter(c)\n",
    "            print(self.token_count)\n",
    "        \n",
    "        \n",
    "        def topN(self, N):\n",
    "            print(self.token_count.most_common(N))\n",
    "          \n",
    "   \n",
    "def bigram(doc, N):\n",
    "    \n",
    "    result=[]\n",
    "    c = tokenize(doc)\n",
    "    result = [(c[i],c[i+1]) for i in range(len(c)-1)]\n",
    "    a = Counter(result)\n",
    "    print(colored(\"Bigram and count pair\", \"blue\", attrs=['bold']))\n",
    "    print(a)\n",
    "    print(\"\")\n",
    "    print(colored(\"Top N bigram\", \"blue\", attrs=['bold']))\n",
    "    print(a.most_common(N))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    text=''' There was nothing so VERY remarkable in that; nor did Alice\n",
    "think it so VERY much out of the way to hear the Rabbit say to\n",
    "itself, `Oh dear!  Oh dear!  I shall be late!'  (when she thought\n",
    "it over afterwards, it occurred to her that she ought to have\n",
    "wondered at this, but at the time it all seemed quite natural);\n",
    "but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT-\n",
    "POCKET, and looked at it, and then hurried on, Alice started to\n",
    "her feet, for it flashed across her mind that she had never\n",
    "before seen a rabbit with either a waistcoat-pocket, or a watch to\n",
    "take out of it, and burning with curiosity, she ran across the\n",
    "field after it, and fortunately was just in time to see it pop\n",
    "down a large rabbit-hole under the hedge.\n",
    "'''   \n",
    "    print(colored(\"Output of Question 1:\", \"green\", attrs=['bold']))\n",
    "    print( tokenize(text))\n",
    "    print(\"\")\n",
    "    # Test Question 2\n",
    "    analyzer=Text_Analyzer(text)\n",
    "    print(colored(\"Output of question 2:\", \"green\", attrs=['bold']))\n",
    "    print(colored(\"Words and count pair\", \"blue\", attrs=['bold']))\n",
    "    analyzer.analyze()\n",
    "    print(\"\")\n",
    "    print(colored(\"Top N Words\", \"blue\", attrs=['bold']))\n",
    "    analyzer.topN(5)\n",
    "    \n",
    "    #3 Test Question 3\n",
    "    print(\"\")\n",
    "    print(colored(\"Output of question 3:\", \"green\", attrs=['bold']))\n",
    "    bigram(text, 5)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
